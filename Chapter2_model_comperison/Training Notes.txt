Example invoke


Lst run model based
# %%

def run_qac_path_parallel_notebook(
    taus=(0.1, 0.5, 0.9),
    train_seeds=(53, 274, 1234, 89),
    q_matrices=("bull_bear", "neutral_bear", "bull_neutral"),
    test_seeds=(1, 2, 3, 4),

    # train/test horizon
    train_T_days=30*252,
    train_burn_in_months=50,
    train_start_date="2000-01-03",
    test_T_days=20*252,
    test_burn_in_months=0,
    test_start_date="2000-01-03",

    # training
    episodes=30,
    batch_size=128,
    gamma=0.99,
    rho=0.01,
    critic_lr_start=0.005,
    critic_lr_end=0.001,
    actor_lr_start=0.001,
    actor_lr_end=0.0001,
    actor_loss="weighted_quantile",
    critic_type="standard",
    entropy_reg=0.01,
    transaction_cost=0.0021,
    gamma_crra=5.0,

    # output
    save_as_file="path_run20260209",
    checkpoints_dir="./training_outcome",
    parallel_max_workers=3,
    evaluate=True,
    test_tag="test",

    test_use_mean_action=True,
    test_store_daily_weights=True,
    add_tic_date=False,
):

    ipy = get_ipython()

    tau_str = ",".join(str(t) for t in taus)
    train_seed_str = ",".join(str(s) for s in train_seeds)
    q_str = ",".join(q_matrices)
    test_seed_str = ",".join(str(s) for s in test_seeds)

    cli = (
        f"--parallel_batch True "
        f"--parallel_taus {tau_str} "
        f"--parallel_seeds {train_seed_str} "
        f"--parallel_q_matrices {q_str} "
        f"--parallel_max_workers {parallel_max_workers} "
        f"--episodes {episodes} "
        f"--batch_size {batch_size} "
        f"--gamma {gamma} "
        f"--rho {rho} "
        f"--critic_lr_start {critic_lr_start} "
        f"--critic_lr_end {critic_lr_end} "
        f"--actor_lr_start {actor_lr_start} "
        f"--actor_lr_end {actor_lr_end} "
        f"--actor_loss {actor_loss} "
        f"--critic_type {critic_type} "
        f"--entropy_reg {entropy_reg} "
        f"--transaction_cost {transaction_cost} "
        f"--gamma_crra {gamma_crra} "
        f"--checkpoints_dir {checkpoints_dir} "
        f"--save_as_file {save_as_file} "
        f"--add_tic_date {str(add_tic_date)} "
        f"--evaluate {str(evaluate)} "
        f"--test_tag {test_tag} "
        f"--test_use_mean_action {str(test_use_mean_action)} "
        f"--test_store_daily_weights {str(test_store_daily_weights)} "
        f"--train_T_days {train_T_days} "
        f"--train_burn_in_months {train_burn_in_months} "
        f"--train_start_date {train_start_date} "
        f"--test_T_days {test_T_days} "
        f"--test_burn_in_months {test_burn_in_months} "
        f"--test_start_date {test_start_date} "
        f"--test_seeds {test_seed_str} "
    )

    print("\nRUNNING:\n", "Agent_model_based_path.py", cli, "\n")

    ipy.run_line_magic("run", f"Agent_model_based_path.py {cli}")


# Example:
run_qac_path_parallel_notebook()


def run_qac_batch(taus, seeds):
    ipython = get_ipython()
    for tau in taus:
        for seed in seeds:
                file_name = 'final_Q_neutral_bear_original_{}_{}'.format(
                    str(seed), str(tau).replace('.', '')
                )

                cli_args = (
                    f'--learning_tau {tau} --gamma 0.96 '
                    f'--save_as_file {file_name} '
                    f'--episodes 10 '
                    f'--add_tic_date True '
                    f'--critic_lr_start 0.001 '
                    f'--critic_lr_end 0.0001 '
                    f'--rho 0.02 '
                    f'--mode train '
                    f'--seed {seed} '
                    f'--actor_loss weighted_quantile '
                    f'--actor_lr_start 0.0001 '
                    f'--actor_lr_end 0.00001 '
                )

                print(f"\nRunning: Agent_model_based.py {cli_args}")
                ipython.run_line_magic('run', f'Agent_model_based.py {cli_args}')

# Example call:
taus = [0.9,0.1, 0.5]
seeds = [53,274,1234,89]    


when we invoke the model we see the current parameters such as
{
  "render_each": 100,
  "evaluate": true,
  "seed": 53,
  "episodes": 10,
  "gamma": 0.96,
  "learning_tau": 0.9,
  "tau_levels": 10,
  "critic_lr_start": 0.001,
  "critic_lr_end": 0.0001,
  "actor_lr_start": 0.0001,
  "actor_lr_end": 1e-05,
  "rho": 0.02,
  "entropy_reg": 0.01,
  "transaction_cost": 0.001,
  "initial_wealth": 1.0,
  "r_f": 1.001,
  "gamma_crra": 3.0,
  "n_scen": 64,
  "batch_size": 512,
  "checkpoints_dir": "./training_outcome",
  "load_weights": false,
  "load_from_file": "test",
...

# RS-VAR parameters

    Phi_k = np.tile(np.array([[0.15, 0.10],
                          [0.10, 0.15]], dtype=float), (3, 1, 1))

#original base
    const_k = np.array([
        [ 0.0040,  0.0030],   # Bull
        [ 0.0030,  0.0028],   # Neutral
        [-0.0090,  0.0030],   # Bear
    ], dtype=float)


    #original base
    Sigma_k = np.array([
        [[0.0005,  0.00010],
        [0.00010, 0.00045]],   # Bull
        [[0.0018,  0.00000],
        [0.00000, 0.00140]],   # Neutral
        [[0.0050, -0.00300],
        [-0.00300, 0.00200]],  # Bear
    ], dtype=float)



    Q_bull_bear = np.array([
    [0.74, 0.02, 0.24],
    [0.10, 0.82, 0.08],
    [0.30, 0.02, 0.68]],dtype=float)  

    Q_neutral_bear = np.array([
    [0.82, 0.08, 0.10],
    [0.02, 0.68, 0.30],
    [0.02, 0.24, 0.74]],dtype=float)   

    Q_bull_neutral = np.array([
    [0.74, 0.24, 0.02],
    [0.30, 0.68, 0.02],
    [0.10, 0.08, 0.82]],dtype=float)    

    K, N = const_k.shape

    # previous risky weight grid
    Wgrid = np.array([
        [0.00, 0.00],
        [1.00, 0.00],
        [0.00, 1.00],
        [0.50, 0.50],
        [0.20, 0.20],
        [0.35, 0.35],
        [0.75, 0.25],
        [0.25, 0.75],
        [0.60, 0.20],
        [0.20, 0.60],
        [0.00, 0.10],
        [0.10, 0.00],
        [0.30, 0.00],
        [0.00, 0.30],
        [0.60, 0.00],
        [0.00, 0.60],
    ], dtype=float)

    # log-return grid
    axes_list = [np.round(np.linspace(-0.04, 0.04, 7, dtype=float), 3) for _ in range(N)]

Where R_grid is just the Cartesian product


Model Parameters


parser.add_argument('--transaction_cost', type=float, default=1e-3) # 1e-5 small
parser.add_argument('--initial_wealth', type=float, default=1.0)
parser.add_argument('--r_f', type=float, default=1.001)
parser.add_argument('--gamma_crra', type=float, default=3.0) # Quantile function is invariant to any monotonic transformation, but lets align with Expected case
parser.add_argument('--n_scen', type=int, default=64)

# minibatch
parser.add_argument('--batch_size', type=int, default=512, help="Minibatch size for training; <=0 uses full batch")

# saving
parser.add_argument('--add_tic_date', type=str2bool, default=False)
parser.add_argument('--reward_scaling', type=float, default=1)
parser.add_argument('--batches_per_epoch', type=float, default=200)

parser.add_argument('--actor_loss', type=str, nargs='?', default='weighted_quantile',
    choices=['advantage', 'is_negative', 'weighted_quantile','original'])
parser.add_argument('--critic_type', type=str, nargs='?', default='standard',
    choices=['standard', 'monte_carlo_dropout', 'bayesian'])


Used model set up Dirichlet
# -*- coding: utf-8 -*-
"""
Created on Tue Nov  7 13:54:49 2023

@author: Attila

 The code is based on Lukas Janasek's work (logic, functions, methodology).
 The main difference is the Actor function. This case it is continious. The output is mu and log sigma,
 which is transformed later on with exp. Before the output layer, the input is flattened.


"""
import logging
import os
import tensorflow_probability as tfp

import tensorflow as tf

import numpy as np

EPSILON = 0.0000000001


def update_target(model_target, model_ref, rho=0.001):
    model_target.set_weights([rho * ref_weight + (1 - rho) * target_weight
                              for (target_weight, ref_weight) in
                              list(zip(model_target.get_weights(), model_ref.get_weights()))])


class Actor:
    '''
    Actor network for Continious case
    Defining a neural network that models a multivariate normal distribution
    Outputs are mu and log(sigma)--> not to loose information with softplus
    Stock_dimension: Number of stocks
    Flattened : We need to flatten cause the output is a matrix.

    '''

    def __init__(self, state_shape, stock_dimension,dropout_rate=0.1, args=None):
        # stock_dimension: Number of stocks
        # shape: (features,stocks)
        self.args = args
        y_inputs = tf.keras.layers.Input(shape=state_shape, dtype=tf.float32)
        # normalized = LayerNormalization(axis=1)(y_inputs)
        # conv = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(normalized)

        o1 = tf.keras.layers.Dense(32, activation=tf.nn.leaky_relu,
                                   kernel_initializer=tf.keras.initializers.HeNormal(),
                                   kernel_regularizer=tf.keras.regularizers.L2(l2=0.0001))(
            y_inputs)  # too high L2. it should be 0.001
        # normalized = LayerNormalization(axis=1)(o1)
        #o1 = tf.keras.layers.Dropout(rate=dropout_rate)(o1)
        o1 = tf.keras.layers.Dense(32, activation=tf.nn.leaky_relu,
                                   kernel_initializer=tf.keras.initializers.HeNormal(),
                                   kernel_regularizer=tf.keras.regularizers.L2(l2=0.0001))(o1)  # 0.01
        last_init = tf.keras.initializers.HeNormal()
        #o1 = tf.keras.layers.Dropout(rate=dropout_rate)(o1)
        flattened_input = tf.keras.layers.Flatten()(o1)
        alphas = tf.keras.layers.Dense(stock_dimension, activation='softplus', kernel_initializer=last_init)(flattened_input)

        self.model = tf.keras.Model(inputs=y_inputs, outputs=alphas)

    def call_action(self, state):
        # TODO : actually we should multiply (with less then 1) rather than add a number. We ll get
        # nan-s if we do not add +1. Also we may experience overflow
        alphas = self.model(state)

        variance_control = 0.2 #0.5 was great, 1: lets delete this the higher the value, the lower the variance, must be positive 0.7 defailt and i used 0.5 for 26
        # TODO: calculate mean shift bias
        # we should use Floor rather alphas = tf.clip_by_value(alphas, 1e-2, 1e6)  :No global mean bias
        # Assign -1000 to all elements
        # batch_size = tf.shape(mu)[0]
        # indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], 4)], axis=1)
        # updates = tf.fill([batch_size], 1000.0)
        # mu = tf.tensor_scatter_nd_update(mu, indices, updates)

        return alphas + variance_control

    def sample_action(self, state):
        '''
        Sampling actions (portfolio weights)
        '''
        alphas = self.call_action(state)
        sampled_raw_actions = tfp.distributions.Dirichlet(alphas).sample()

        return sampled_raw_actions

    def get_log_action_prob(self, state, sampled_raw_actions):
        '''
        'sampled_raw_actions': We use this as an input and not sampling it here cause we would get different result.
        The sampled_raw_actions is needed before the learn function to do step.
        This sampled action comes from the same distibution(mu,sigma are the same here and sample_action funtion) so
        officially we should not loose anything if there is no training part before.
        When we are under the Gradient Tape we should
        learn the distribution via 'sampled_raw_actions' cause it describes the distribution.
        Question(1): would it change the training if the sampling (sampled_raw_actions part) would be under the gradient tape?

        '''
        alphas = self.call_action(state)
        dirichlet_dist = tfp.distributions.Dirichlet(alphas)
        log_prob_actions = tf.math.log(dirichlet_dist.prob(sampled_raw_actions) + EPSILON) # EPSILON Seems not big enough--> huge gradients
        log_prob_actions = tf.expand_dims(log_prob_actions, axis=1)

        entropy = dirichlet_dist.entropy()
        mean_entropy = tf.reduce_mean(entropy)

        return log_prob_actions, mean_entropy

    def get_trainable_vars(self):
        vars = self.model.trainable_variables

        return vars


def get_critic_model(state_shape, tau_levels=10, dirictlet=False, args=None):
    # stock_dimension: Number of stocks

    y_inputs = tf.keras.layers.Input(shape=state_shape, dtype=tf.float32)

    # normalized = LayerNormalization(axis=1)(y_inputs)
    # conv = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(normalized)

    o = tf.keras.layers.Dense(32, activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.HeNormal(),
                              kernel_regularizer=tf.keras.regularizers.L2(l2=0.0001))(y_inputs) #l2=0.0001 original
    # normalized = LayerNormalization(axis=1)(o)
    o = tf.keras.layers.Dense(32, activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.HeNormal(),
                              kernel_regularizer=tf.keras.regularizers.L2(l2=0.0001))(o)

    last_init = tf.keras.initializers.GlorotNormal() #tf.keras.initializers.GlorotNormal(): should be better than HeNormal here
    output = tf.keras.layers.Dense(tau_levels, activation='linear', kernel_initializer=last_init)(o)

    model = tf.keras.Model(y_inputs, output)
    return model


def get_monte_carlo_droput_critic(state_shape, tau_levels=10, dirictlet=False,dropout_rate=0.1, args=None):
    '''
    MC dropout is nothing else just sampling from trainied weights. Consequently, we let training=True
    during interference and add droput layer.
    Shapley values package is not competible with leaky relu, I changed here to relu.
    TODO: figoure this out
    '''

    y_inputs = tf.keras.layers.Input(shape=state_shape, dtype=tf.float32)

    # normalized = LayerNormalization(axis=1)(y_inputs)
    # conv = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(normalized)

    o = tf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.HeNormal(),
                              kernel_regularizer=tf.keras.regularizers.L2(l2=0.0001))(y_inputs) # 0.0005
    # normalized = LayerNormalization(axis=1)(o)
    #o = tf.keras.layers.BatchNormalization()(o)
    o = tf.keras.layers.Dropout(rate=dropout_rate)(o)
    o = tf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.HeNormal(),
                              kernel_regularizer=tf.keras.regularizers.L2(l2=0.0001))(o) # 0001 default
    #o = tf.keras.layers.BatchNormalization()(o)
    o = tf.keras.layers.Dropout(rate=dropout_rate)(o)
    last_init = tf.keras.initializers.GlorotNormal()
    output = tf.keras.layers.Dense(tau_levels, activation='linear', kernel_initializer=last_init)(o)

    model = tf.keras.Model(y_inputs, output)
    return model


'''

def prior(kernel_size, bias_size, dtype=None):
    """
    Define the prior distribution as a multivariate normal with mean 0 and variance 1.
    """
    n = kernel_size + bias_size
    prior_model = tf.keras.Sequential(
        [
            tfp.layers.DistributionLambda(
                lambda t: tfp.distributions.MultivariateNormalDiag(
                    loc=tf.zeros(n),scale_diag=tf.ones(n) * 2.0
                )
            )
        ]
    )
    return prior_model
'''

tfd = tfp.distributions

def prior(kernel_size, bias_size=0, dtype=None):
    """
    Returns a Keras model that, when called, outputs a tfp.Distribution
    (MultivariateNormalDiag) with trainable loc and fixed scale=2.0.
    """
    n = kernel_size + bias_size
    return tf_keras.Sequential([
        # Step 1: A VariableLayer that creates a trainable vector (our 'mu')
        tfp.layers.VariableLayer(
            shape=(n,),
            dtype=dtype,
            initializer=tf.keras.initializers.GlorotNormal(),
            name="prior_mu"
        ),
        # Step 2: Convert that vector into a TFP distribution
        tfp.layers.DistributionLambda(
            lambda mu: tfd.MultivariateNormalDiag(
                loc=mu,
                scale_diag=tf.ones([n], dtype=dtype) * 2.0 #2
            ),
            name="prior_dist"
        )
    ])
def posterior(kernel_size, bias_size=0, dtype=None):
    """
    Returns a Keras model that outputs a tfp.distributions.MultivariateNormalTriL
    with fully trainable loc and lower-triangular scale.
    """
    n = kernel_size + bias_size
    return tf_keras.Sequential([
        # Step 1: A single trainable vector that parameterizes (loc + raw scale_tril)
        tfp.layers.VariableLayer(
            tfp.layers.MultivariateNormalTriL.params_size(n),
            dtype=dtype,
            initializer=tf.keras.initializers.GlorotNormal(),
            name="posterior_params"
        ),
        # Step 2: Convert that vector into a TFP distribution
        # (This layer directly creates a MultivariateNormalTriL)
        tfp.layers.MultivariateNormalTriL(n, name="posterior_dist")
    ])


def get_bayesian_critic_model(state_shape, tau_levels=10):
    if isinstance(state_shape, tuple):
        state_shape = tf.TensorShape(state_shape)

    y_inputs = tf_keras.layers.Input(shape=state_shape, dtype=tf.float32)
    # 1st DenseVariational
    x = tfp.layers.DenseVariational(
        units=32,
        make_prior_fn=prior,          # returns the prior model
        make_posterior_fn=posterior,  # returns the posterior model
        kl_weight=64/1400,
        activation='relu',
        name="dense_variational_1"
    )(y_inputs)

    # 2nd DenseVariational
    x = tfp.layers.DenseVariational(
        units=16,
        make_prior_fn=prior,
        make_posterior_fn=posterior,
        kl_weight=64/1400,
        activation='relu',
        name="dense_variational_2"
    )(x)

    # Final deterministic Dense
    outputs = tf_keras.layers.Dense(tau_levels, activation='linear')(x)

    model = tf_keras.Model(y_inputs, outputs)
    return model


class QACDirichletAgent:
    def __init__(self, state_shape, stock_dimension, args):
        # stock_dimension: Number of stocks
        tau_levels = [t / args.tau_levels for t in range(1, args.tau_levels)]

        self.name = 'qac_agent'

        self.N = len(tau_levels)
        self.Nq = self.N

        self.learning_tau = args.learning_tau
        self.tau_levels = tf.convert_to_tensor(tau_levels, dtype='float32')
        self.entropy_reg = args.entropy_reg
        self.args = args
        self.learning_tau_index = tau_levels.index(args.learning_tau)
        self.rho = args.rho
        self.critic_loss_history = []
        self.actor_loss_history = []
        self.td_error_history = []
        self.entropy_history = []
        self.diff_penalty = []
        self.cost_history = []


        # self.y=[]
        self.v = []

        self.actor_network = Actor(state_shape, stock_dimension)
        if args.critic_type == 'monte_carlo_dropout':
            self.critic_network = get_monte_carlo_droput_critic(state_shape, self.N)
            self.critic_target = get_monte_carlo_droput_critic(state_shape, self.N)
        elif args.critic_type == 'bayesian':
            self.critic_network = get_bayesian_critic_model(state_shape, self.N)
            self.critic_target = get_bayesian_critic_model(state_shape, self.N)
        else:  # Default to standard critic model
            self.critic_network = get_critic_model(state_shape, self.N)
            self.critic_target = get_critic_model(state_shape, self.N)
            
        self.critic_target.set_weights(self.critic_network.get_weights())

        self.gradient_magnitudes_critic_per_episode = [] # dont need
        self.gradient_magnitudes_actor_per_episode = []

        self.gamma = tf.constant(args.gamma, dtype='float32')

        critic_decay = tf.keras.optimizers.schedules.PolynomialDecay(
            args.critic_lr_start,
            args.episodes, # 1406/batch * episodes: wrong set up.... use i.e 1000
            end_learning_rate=args.critic_lr_end,
            power=1.5,  # it can cause NaNs, be careful with learning rates. In a non stationary environment we should not use decay rate
        )
        self.critic_optimizer = tf.keras.optimizers.Adam(critic_decay)

        actor_decay = tf.keras.optimizers.schedules.PolynomialDecay(
            args.actor_lr_start,
            args.episodes,#args.episodes,500: tried:smoother outcome
            end_learning_rate=args.actor_lr_end,
            power = 1.5,
        )
        self.actor_optimizer = tf.keras.optimizers.Adam(actor_decay)

    def get_mean_of_action(self, s):
        alphas = self.actor_network.call_action(s)
        mean_share = alphas / tf.reduce_sum(alphas, axis=1, keepdims=True)

        return mean_share


    @tf.function
    def update_weights(self, s, sn, a_raw, r,conditional_prob):
            conditional_prob_sum = tf.reduce_sum(conditional_prob) # noramlize with the number of shocks
            # we dont want to artifically depend on the number of shocks
            # sn: next state
            # s: current state
            # a_raw: conditional action probability based on current state 's'
            # r: reward (stochastic) conditional on the eps and k'
            # conditional probability from k-->k'
            vn = self.critic_target(sn)
            y = r + self.gamma * vn
            increased_order_loss_weight = 5.0
            with tf.GradientTape() as critic_tape:
                v = self.critic_network(s)
                error = y - v

                abs_error = tf.math.abs(error)
                is_negative = tf.where(tf.math.less(error, 0.0), 1.0, 0.0)
                q_order_loss = tf.reduce_mean(tf.maximum(0.0, v[:, :-1] - v[:, 1:] + EPSILON))*increased_order_loss_weight
                #q_order_loss = tf.reduce_mean(tf.nn.softplus(v[:, :-1] - v[:, 1:]) * increased_order_loss_weight)

                loss = tf.math.multiply(tf.math.abs(tf.math.subtract(self.tau_levels, is_negative)), abs_error)
                per_sample = tf.reduce_sum(loss, axis=1, keepdims=True) 
                critic_loss = (tf.reduce_sum(conditional_prob * per_sample)/conditional_prob_sum)+q_order_loss

            critic_grad = critic_tape.gradient(critic_loss, self.critic_network.trainable_variables)
            # critic_grad = [tf.clip_by_norm(grad, 10) for grad in critic_grad]

            self.critic_optimizer.apply_gradients(
                zip(critic_grad, self.critic_network.trainable_variables)
            )   
        
            v = self.critic_network(s)
            error = y - v
            #error = error[:, self.learning_tau_index: self.learning_tau_index + 1]
            #is_negative = tf.where(tf.math.less(error, 0.0), 1.0, 0.0)
            scale = 1
            if self.args.actor_loss=='weighted_quantile':
                t = self.tau_levels[self.learning_tau_index]
                error = error[:, self.learning_tau_index:self.learning_tau_index + 1]
                #is_negative = tf.where(error < 0.0, 1.0, 0.0)
                #t_weight = t - is_negative
                t_weight = tf.where(error < 0.0, 1.0 - t, t)

            elif self.args.actor_loss == 'advantage':
                error = error[:, self.learning_tau_index: self.learning_tau_index + 1]
            elif self.args.actor_loss == 'is_negative':
                error = error[:, self.learning_tau_index: self.learning_tau_index + 1]
                is_negative = tf.where(tf.math.less(error, 0.0), 1.0, 0.0)
                #is_positive = tf.where(error > 0.0, 1.0, 0.0)  # reinforce hitting upside
            elif self.args.actor_loss =='expectation':
                avg_error = tf.reduce_mean(error, axis=1, keepdims=True)

            elif self.args.actor_loss == 'power':
                    error = error[:, self.learning_tau_index: self.learning_tau_index + 1]
                    #t = self.tau_levels[self.learning_tau_index]
                    t = tf.cast(self.tau_levels[self.learning_tau_index], tf.float32)

                    eta = tf.where(
                        tf.equal(t, 0.1),
                        tf.constant(-2.0, dtype=tf.float32),
                        tf.where(
                            tf.equal(t, 0.9),
                            tf.constant(2.0, dtype=tf.float32),
                            tf.constant(0.0, dtype=tf.float32)
                        )
                        )
                    weight_pow = tf.where(
                            eta >= 0,
                            tf.pow(t, 1 / (1 + tf.abs(eta))),
                            1 - tf.pow(1 - t, 1 / (1 + tf.abs(eta)))
                        )

            with tf.GradientTape() as actor_tape:
                log_prob, entropy = self.actor_network.get_log_action_prob(state=s, sampled_raw_actions=a_raw)
                if self.args.actor_loss=='is_negative':   
                    #actor_loss = tf.math.reduce_mean(log_prob * scale * is_negative) - self.args.entropy_reg * entropy
                    actor_loss = tf.reduce_mean(tf.reduce_sum(log_prob * scale * is_negative, axis=1))#- self.args.entropy_reg * entropy
                elif self.args.actor_loss=='weighted_quantile':
                    per_sample_actor = tf.reduce_sum(log_prob * error * t_weight, axis=1, keepdims=True) # we dont scale it up now for siplicity
                    #actor_loss = -tf.reduce_mean(log_prob * error * t_weight)- self.args.entropy_reg * entropy
                    #actor_loss = -tf.reduce_mean(tf.reduce_sum(log_prob * error * t_weight, axis=1)) #- self.args.entropy_reg * entropy
                    actor_loss = - tf.reduce_sum(conditional_prob * per_sample_actor) / conditional_prob_sum         # <<< changed

                elif self.args.actor_loss=='advantage':
                    #actor_loss = -tf.math.reduce_mean(log_prob * error)- self.args.entropy_reg * entropy
                    actor_loss = -tf.reduce_mean(tf.reduce_sum(log_prob * error, axis=1)) #- self.args.entropy_reg * entropy
                elif self.args.actor_loss =='expectation':
                    actor_loss = -tf.reduce_mean(tf.reduce_sum(log_prob * avg_error, axis=1))
                elif self.args.actor_loss =='power':
                    actor_loss = -tf.reduce_mean(tf.reduce_sum(log_prob * error*weight_pow, axis=1))

                    

            actor_vars = self.actor_network.get_trainable_vars()
            actor_grad = actor_tape.gradient(actor_loss, actor_vars)
            # actor_grad = [tf.clip_by_norm(grad, 1) for grad in actor_grad]
            
            self.actor_optimizer.apply_gradients(
                zip(actor_grad, actor_vars)
            )

            return actor_loss, critic_loss  
    '''
        def update_weights(self, s, sn, a_raw, r, w=None):
            if w is None:
                w = tf.ones_like(r)                     # (B,1)
            w_mean = tf.reduce_mean(w)
            vn = self.critic_target(sn)
            y  = r + self.gamma * vn

            with tf.GradientTape() as critic_tape:
                v = self.critic_network(s)
                err = y - v
                abs_err = tf.abs(err)
                is_neg  = tf.cast(err < 0.0, tf.float32)
                pin_w   = tf.abs(self.tau_levels - is_neg)
                per_sample = tf.reduce_sum(pin_w * abs_err, axis=1, keepdims=True)  # (B,1)
                loss_q = tf.reduce_mean(w * per_sample) / (w_mean + EPSILON)

                order_pen = tf.reduce_mean(tf.maximum(0.0, v[:, :-1] - v[:, 1:] + 1e-8)) * 5.0
                critic_loss = loss_q + order_pen

            critic_grad = critic_tape.gradient(critic_loss, self.critic_network.trainable_variables)
            self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic_network.trainable_variables))

            v  = self.critic_network(s)
            vn = self.critic_target(sn)
            y  = r + self.gamma * vn
            err_tau = y[:, self.learning_tau_index:self.learning_tau_index+1] - \
                    v[:, self.learning_tau_index:self.learning_tau_index+1]

            if self.args.actor_loss == 'weighted_quantile':
                t = self.tau_levels[self.learning_tau_index]
                t_weight = tf.where(err_tau < 0.0, 1.0 - t, t)
                adv = err_tau * t_weight
            elif self.args.actor_loss == 'advantage':
                adv = err_tau
            else:
                adv = err_tau

            with tf.GradientTape() as actor_tape:
                log_prob, entropy = self.actor_network.get_log_action_prob(s, a_raw)
                logp_sum = tf.reduce_sum(log_prob, axis=1, keepdims=True)  # (B,1)
                actor_loss = -tf.reduce_mean(w * logp_sum * tf.stop_gradient(adv)) / (w_mean + EPSILON) \
                            - self.entropy_reg * tf.reduce_mean(entropy)

            actor_vars = self.actor_network.get_trainable_vars()
            actor_grad = actor_tape.gradient(actor_loss, actor_vars)
            self.actor_optimizer.apply_gradients(zip(actor_grad, actor_vars))

            return actor_loss, critic_loss  
    '''

    def learn(self, transitions):
        # transitions are (s, sn, a_raw, r, q)
        # w: conditional probability of the given regime to regime next
        '''
        s: current state
        sn: next state
        a: raw action, non normalized output from critic--> mismatch?--> learning behaviour?
        r: reward based on transition
        Q: from k -->k' prob i.e from Q matrix
        array([[0.74, 0.24, 0.02],
              [0.3 , 0.68, 0.02],
              [0.1 , 0.08, 0.82]])
        '''
        states, next_states, a_raws, rewards, weights = zip(*transitions)
        s  = tf.convert_to_tensor(np.array(states), dtype=tf.float32)
        sn = tf.convert_to_tensor(np.array(next_states), dtype=tf.float32)
        a  = tf.convert_to_tensor(np.array(a_raws), dtype=tf.float32)
        r  = tf.reshape(tf.convert_to_tensor(np.array(rewards), dtype=tf.float32), (-1, 1))
        conditional_prob  = tf.reshape(tf.convert_to_tensor(np.array(weights), dtype=tf.float32), (-1, 1))

        a_loss, c_loss = self.update_weights(s, sn, a, r, conditional_prob)
        update_target(self.critic_target, self.critic_network, self.rho)
        return a_loss.numpy(), c_loss.numpy()

    def act(self, state_batch):
        """
        state_batch: [batch, state_dim]
        returns RAW logits (env will project).
        """
        return self.actor_network.sample_action(state_batch)

    def save_weights(self, path):
        os.makedirs(path, exist_ok=True)
        self.actor_network.model.save_weights(path + "an_y.weights.h5")
        self.critic_network.save_weights(path + "cn.weights.h5")
        self.critic_target.save_weights(path + "ct.weights.h5")

    def load_weights(self, path, raise_error=False):
        try:
            self.actor_network.model.load_weights(path + "an_y.weights.h5")
            self.critic_network.load_weights(path + "cn.weights.h5")
            self.critic_target.load_weights(path + "ct.weights.h5")
        except OSError as err:
            logging.warning("Weights files cannot be found, %s", err)
            if raise_error:
                raise
